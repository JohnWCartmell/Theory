\documentclass[10pt,a4paper]{scrartcl}


\input{../SharedMacros/theorems.macros}
\input{../SharedMacros/ccategories.macros}
\input{../SharedMacros/gats.macros}
\input{../SharedMacros/metagat.macros}
\input{../SharedMacros/ermacros}
\input{../SharedMacros/erdiagram}
\input{../SharedMacros/indexedsets.macros}
\input{../SharedMacros/general.macros}

\usepackage{mathptmx}  % This changes font to roman
\usepackage{anyfontsize}
%\usepackage{amsfonts}
% \usepackage{wasysym}  %why was this here?
\usepackage[margin=3cm]{geometry}
%\usepackage{amssymb}   %This is in ermacros.tex
%\usepackage{enumerate} %This is in ermacros
\usepackage{url}
\usepackage{hyperref}
\usepackage{framed}



% \usepackage{environ}  %This is in ermacros
\NewEnviron{shrunkdisplay}{%
\begin{equation*}
\scalebox{0.9}{$\BODY$}
\end{equation*}
}

\renewcommand{\erpictureFolder}[0]{../SharedPictures}

\setcounter{equation}{0}
%\bibliographystyle{apalike} 
%\bibliographystyle{plainnat}
\bibliographystyle{plain}
\usepackage{bibentry}
\nobibliography*

\newcommand{\displaybibentry}[1]
{\begin{framed}
\bibentry{#1}
\end{framed}
}

\title{Review of Data/Database Specification Literature}
\author{John Cartmell\footnote{john.w.cartmell gmail.com}\\ \normalsize{\textit{Ad Otium}}}
\begin{document}
\maketitle

\section{Codd and the Relational Model of Data}
\displaybibentry{CoddBook1990}
From Codd's 1990 book \cite{CoddBook1990}

\begin{quote}
The relational model is solidly based on two parts of mathematics: first-
order predicate logic and the theory of relations.
\end{quote} 

In 1970, in \\
\displaybibentry{Codd1970}
Codd introduces the relational model of data and introduces the idea of normal form.

\displaybibentry{Codd1971} 
In 1971, the terms `functional dependency' and  `third normal form' are introduced 
in an IBM techical report published in a now out of print book (also unavailable on Amazon).

The following dedication is writ large in Codd's 1990 book:
\begin{quote}
To fellow pilots and aircrew
in the Royal Air Force
during World War II
and the dons at Oxford.
These people were the source of my determination to
fight for what I believed was right during the ten or
more years in which government, industry, and
commerce were strongly opposed to the relational
approach to database management.
\end{quote}


\section{Chen}
\displaybibentry{Chen1976} 


Chen introduces the entity-relationship model of data. 



According to Chen this model

\begin{quote}
 can achieve a high degree of data independence and is based on set theory and relation theory,
\end{quote}
\begin{quote}
 can be used as a basis for a unified view of data,
\end{quote}
\begin{quote}
adopts a top-down approach, utilizing the semantic information to organize
data in entity/relationship relations [whereas, according to Chen, Codds approach may be viewed as a bottom-up approach in database design]
\end{quote}
and may be viewed
\begin{quote}
... as a generalization or extension of existing models.
\end{quote}

In support of the model:
\begin{quote}
A special diagrammatric technique, the entity-relationship diagram, is introduced as a tool for database design. 
\end{quote}

According to Chen, Codd's approach:
\begin{quote}
may be viewed as a bottom-up approach in database design 
\end{quote}
whereas 
\begin{quote}
The entity-relationship model adopts a top-down approach, utilizing the semantic information to organize
data in entity/relationship relations.
\end{quote}

Chen   introduces the idea of entities being dependent on binary relationships 
with others for both their identification and thier existence:
\begin{quote}
Theoretically, any kind of relationship may be used to identify entities. For
simplicity, we shall restrict ourselves to the use of only one kind of relationship:
the binary relationships with 1:n mapping in which the existence of the n entities
on one side of the relationship depends on the existence of one entity on the other
side of the relationship. For example, one employee may have n ( = 0, 1, 2, . . .)
dependents, and the existence of the dependents depends on the existence of the
corresponding employee.
This method of identification of entities by relationships with other entities can
be applied recursively until the entities which can be identified by their own attribute
values are reached. For example, the primary key of a department in a
company may consist of the department number and the primary key of the
division, which in turn consists of the division number and the name of the company.
\end{quote}

\section{Fagin}
\subsection{1977 Fourth Normal Form}
\displaybibentry{Fagin1977}
Fagin \cite{Fagin1977} introduces fourth normal form(4NF) and multivalued dependencies.

\subsection{1979 Projection-Join Normal Form aka Fifth Normal Form}
The fifth normal form was first described by Ronald Fagin in his 1979  paper Normal forms and relational database operators\cite{Fagin1979}. He christened it `projection-join normal form' and gives reasons for his preference for this term.

\section{Zaniolo 1982}
\displaybibentry{zaniolo1982}
Very well written paper. Good place to go for a reminder of normal forms.
\begin{itemize}
\item uses the terms \textit{attribute} and \textit{domain} 
\item to each attribute of a relation there is an underlying domain
\item resume of 3NF and BCNF
\item the representation principle (by example)
\item example (3.1) 3NF not BCNF
\item example (3.2)(3.3) now in BCNF and satisfying representation principle
\item example (3.5)(3.6) in BCNF but not satisying representation principle
\item example (3.4)(3.5) satisfying representation principle
\item clarifications of 3NF and BCNF and inter-relationship
\item new normal form -- elementary key normal form (EKNF)
\item example (3.4) (3.5) is in EKNF but not BCNF
\item Bernsteins algorthm, which is known to produce schemas in 3NF, does actually produce EKNF
\end{itemize}

\textbf{Comments}
The example (3.4) (3.5), if it were to be modelled in ER, would necessitate an entity having a derived relationship as identifying feature.

This is therefore another example of requiring commutative diagrams in ER modelling -- since we have to model the derived relationship
to flag it as identifying.

I emailed Zaniolo some time ago and recieved no reply. My question was:
\begin{quote}
In the (3.1) example you have DA(D\#,MGID,ACC\#)  

and you have FDs f1: D\# $\rightarrow$ MGID and f2:MGID $\rightarrow$ D\#. As you say later, this schema is not in EKNF. 

I would like to clarify that I understand your definitions (though they do seem perfectly clear!). 
To this end please could you clarify that if your 3.1 example is modified by the addition of an attribute,
BALANCE say, to the schema to get DA(D\#,MGID,ACC\#,BALANCE) 
with two keys (D\#, ACC\#) and  (MGID, ACC\#) unchanged and FDs f1 and f2 
as before then this extended schema is in EKNF because 
the attributes MGID and ACC\# are now elementary key attributes? 
{\color{red} 12 March 2019 - reconsidered this but I think my question stands and I think this invalidates Zaniolo's definition. Further and significant thoughts on this in journal entry Thurs 13 March 2019. }
{\color{blue} 26 June 2019 - I agree with reconsideration. Zaniola's defintion is invalidated by this.}
\end{quote}

\begin{center}
\input{\erpictureFolder/zanioloExample1}
\end{center}


\textbf{Zaniolo Second Example}
This is an example of a relational schema which is in Elementary Key Normal Form but not in BCNF. When expressed in ER terms we have:
\begin{center}
\input{\erpictureFolder/zanioloExample2}
\end{center}
\textit{The `telephone.area' relationship shown here is identifying. Therefore so to is the `telephone.at' relationship.
The relationsal model is therefore not in BCNF. It is not possible to express such a  model in my formalisation of ER model as I  have it currently in my Relational Data Design document. It would be possible in Johnstone I think. Neither is it supported in Oracle. My ERScript software doesn't support it either.} 


Compare this notion of elementary key with Ling and Goh'definition of ``essential'' and ``weakly-essential'' attributes. What is the diffeence between Ling \& Goh's Improved 3NF and Zaniolo's Elementary Key Normal Form?

\section{Cartmell 1986}
\displaybibentry{CartmellNetworkDataModel}

\section{Cartmell \& Alderson 1997}
\displaybibentry{CartmellScopePaper}

\section{Carboni, Lack and Walters --  Extensive and Distributive Categories}
\displaybibentry{Carboni1993}

\begin{quote}
In a category with sums and pullbacks along injections, sums are
said to be disjoint if the pullback of the injections of a binary sum is the initial
object, and all injections are monic.
\end{quote}

and
\begin{quote}
In a category with finite sums and pullbacks along their injections,
a coproduct diagram

\begin{displaymath}
\begin{array} {cp{0.5cm}cp{0.5cm}c}
\Rnode{X1}{X_1} && \Rnode{sumX1X2}{X_1+X_2} && \Rnode{X2}{X_2} \\
\end{array}
\ncarr{X1}{sumX1X2}
\naput{x_1}
\ncarr{X2}{sumX1X2}
\nbput{x_2}
\end{displaymath}

is said to be universal if pulling it back along any morphism into $X_1 + X_2$, gives a
coproduct diagram.
\end{quote}
So if $f: A \morph x_1+X_2$ and $A_1$ is the pullback of $X_1$ along $f$ and $A_2$ is the pullback of $X_2$ along $f$
as here:
\begin{displaymath}
\begin{array} {cp{0.5cm}cp{0.5cm}c}
\Rnode{A1}{A_1} && \Rnode{A}{A} && \Rnode{A2}{A_2} \\[1.5cm]
\Rnode{X1}{X_1} && \Rnode{sumX1X2}{X_1+X_2} && \Rnode{X2}{X_2} \\
\end{array}
\ncarr{A}{sumX1X2}
\nbput{f}
\ncarr{A1}{X1}
\nbput{f_1}
\ncarr{A2}{X2}
\nbput{f_2}
\ncarr{A1}{A}
\naput{a_1}
\ncarr{A2}{A}
\nbput{a_2}
\ncarr{X1}{sumX1X2}
\naput{x_1}
\ncarr{X2}{sumX1X2}
\nbput{x_2}
\end{displaymath}
then
\begin{displaymath}
\begin{array} {cp{0.5cm}cp{0.5cm}c}
\Rnode{A1}{A_1} && \Rnode{A}{A} && \Rnode{A2}{A_2} \\
\end{array}
\ncarr{A1}{A}
\naput{a_1}
\ncarr{A2}{A}
\nbput{a_2}
\end{displaymath}
is a coproduct diagram.

This is effectively relationship specialisation, in CAIS-A
 data model $f_1$ an d $f_2$ are specialisations of $f$: 

\begin{center}
\input{\erpictureFolder/relationshipSpecialisation}
\end{center}
In Toolbuilder $f$ would be modelled as an aggregation of $f_1$ and $f_2$ i.e. as 
a derived relationship.
\section {Diskin and Cadish 1996}
\displaybibentry{Diskin1996DatabaseDesign}

\begin{quote}
...it seems for us that
the current situation in relating category theory with DB theory and practice is very similar to the 16th
century interaction of differential and integral calculi, on one hand, with mechanics and engineering on
the other. Thus, being excited with our discovery, and having in mind the distinctive features of our time
... we have decided to begin propagating our observations with a declarative document in
a manner of a brief manifesto.
\end{quote}

\section {Piessens and Steegmans}
\subsection{Categorical Data Specifications, 1995}
The paper \cite{piessens1995} defines data specifications and also MD-sketches.
Introduces the term \textit{source} for a family of morphisms with a common domain and \textit{double source} for two such families with a common domain.
Introduces the term \textit{mono source} 	for what is elsewhere also called jointly monic (note nLab gives both terms) and defines a double source to be \textit{disjoint} if the limit of the double source as a diagram is the initial object.

The definition of MD-sketch is as a \highlight{finite} category with a set S of sources and a set D of double sources. A model is a \cat{FinSet} valued functor taking each source in M to a monic source and each double source in D to a disjoint double source. 
I don't have a feel for what why the definition of MD sketct is as it is but the cosequence is that it is algorithmically decidable whether the model categories of two sketches are equivalent. 



The definition of data specification is as a category \cat{S} along with a specified set of sources M and a function $A: obj(\cat{S}) \morph \cat{Finset}$

A model is a \cat{FinSet} valued functor which takes sources in M to be mono sources along with a natural transformation $\lambda : I \circ M \morph A$ where $I:Obj(\cat{S}) \morph \cat{S}$ is the inclusion functor. Thus $\lambda$ provides a label ($\lambda_E(e)$) for every instance
$e$ of every object $E$ in category \cat{S}.

Finally It is shown that data specifications give rise to MD-sketches and in this way an algorithm is obtained for showing equivalence of model categories of data specifications.


\subsection{Proving semantical equivalence of data specifications, 1997}

\displaybibentry{piessens1997}

This paper deals with finite sketches. Finitely presented categories with cones and cocones which becomne limits and colimits.

Thus it deals with entities and binary relationships but not with attributes. Even so:
\begin{quote}
...we demonstrate that sketches are a very suitable
formalism for making semantic data specifications, as used in database design and software
engineering. Two such data specifications are semantically equivalent iff their model categories in
FinSet are equivalent. 
\end{quote}

A semi-automatic proof
procedure to prove the equivalence of the theories of two sketches is given.



\section {Johnson and others}
\subsection{1994 Commutative Diagramns in Information Modelling} 
\displaybibentry{Johnson93}
Having define a directed graph in which nodes are either types of entity or
else attributes and the edges are functional (many-one) relationships
they argue that there is a composition structure on this graph
and that the resulting category should be thought of as a classifying category
for the ER-model (there is no technical meaning to this term classifying category by the way, 
the term is used descriptively):
\begin{quote}
Since the many-to-one relations in the
model are intended to represent real world many-to-one relations (functions)
there are real world compositions and we argue that these should be represented
in the model. Many of the compositions are free in the sense that formal 
composites can just be added to the model (or indeed left out since such formality
can be added later), but when there is a closed loop of arrows it is important to
determine, by considering the real world semantics, whether the diagram commutes. 
Once this has been done for all possible composites we have constructed
a classifying category for the ER-model.
\end{quote}

Now for the good bit - as true today as when they wrote it twenty five years ago:
\begin{quote}
It is remarkable that extant ER-modelling methodologies have ignored this
question of commuting diagrams. Typically an analyst spends a great deal of
time and effort developing a model and eventually passes it to a programmer
to implement. Often it is important that the resultant program check the 
constraint implied by the commutativity of certain diagrams, but since the analyst
has not recorded which diagrams commute it is up to the less experienced 
programmer to try to reconstruct the intended semantics and to decide whether a
given diagram should commute!
\end{quote}

Next the authors state that certain additional \textit{exactness properties} are required of the category
- no rationale for this at this point:
\begin{quote}
We need a terminal object I, 
and arrows $I \morph A$ will be used to specify instances of the entity A. We
need finite coproducts for two reasons. First, entities often have substructure
which is best indicated by coproducts (so for example in a small retail 
business the entity EMPLOYEE might be the coproduct of the entities DRIVER,
SALESPERSON, CLERICAL STAFF and MANAGEMENT). Secondly, attributes
are fixed sets (so for example PRODUCT NO might be the set of all four digit
numbers -- of course most of these numbers won't be used at any particular point
in time, but the relationship between PRODUCT and PRODUCT NO allows us
to see which ones are currently valid product numbers). Thus attributes are
usually $\sum^nI $ for some n (n = 10000 in our product numbers example). This
is technically very important since the injection $i_k:I \morph \sum^nI$  allows us to
pick out attribute number k from which, if the attribute is a key attribute, we
can obtain information about a particular instance of the corresponding entity.

Finally we need pullbacks, both to allow us to compose relations and to allow
us to access the entity instances with particular attribute values.

Furthermore we expect the coproducts to behave well. They should be
disjoint and universal. Thus in the presence of pullbacks and a 
terminal object
we expect our classifying category to be a \textit{lextensive category} [2] [[Replace this by a citation of my own]].
\end{quote}
Finally, they add that:
\begin{quote}
we expect subobjects to be complemented with I + I acting as
a subobject classifier. 
\end{quote}
and note that an extensive category with this property is called a
\textit{Boolean category} and is necessarily lextensive. 
They note that
\begin{quote}
 the internal logic of the lextensive
classifying category of an information model forms a query language for that
model. Thus the standard queries arise as objects of the classifying category.

The semantics of the information model will be given by lextensive functors
from its classifying category to set. Such functors will necessarily carry the
object representing a query to the set of records which satisfy the query.
\end{quote}
Now we are at the heart of the matter. By the way your can contrast this by my 1985 paper where I am interested in queries that can
be executed in O(log n) time, where n is number of instances involved in the query. The idea there is that there is a lesser
categorical structure which encapsulates there more performant queries which in turn are directly supported by the database structure.
(Remember what telephone directories looked like - open one that has perversely ordered by telephone number instead of by name -
how long will it take to find the number you are looking for - does this distinguish between data (its there somewhere) and information
(its there and I know how to quickly find it).  


I disagree with the following:
\begin{quote}
in order to update the information system, it is usually necessary to
update an instance at each vertex of the diagram and then finally to check that
commutativity has been preserved
\end{quote}
because commutativity should be built into the information content.

\begin{quote}
One particularly difficult problem in dealing with large information systems
is the presentation of different views of the system for different users. The
problem is essentially one of how to partition the system so that users can see
a relatively complete view related to the aspects that are of relevance to them
without having to look at the whole system. The recognition of commuting
diagrams as processes suggests that the best partitioning would be obtained
by choosing a related group of commutative diagrams. This will be developed
in work currently in progress. An early example is shown in Figure 8 where
four views of an information model have been derived by selecting commuting
diagrams from a much larger and more detailed model.
\end{quote}
\textbf{Comment}\\

This is because commuting diagrams indicate locality. Hence composition relationships!

\textbf{Comment}
Regarding the above example of coproducts as what in ER modelling is called inheritance: I suggest could restrict to single inheritance by adding that
there are canonical coproducts (or at least can be presented canonically) and that all objects are domain to at most
one cannonical injection. 

From this it also follows that for each  morphism $1 \morph A$ in the presentation there is a an object
$A_a$ and a cannonical injection $A_a \morph A$ which represents $a$ as a singular subtype of $A$. We can claim that
$A_a$, and, by association, $a$, are universals. 

Bertrand Russell\footnote{Problems of philosophy, page 59 in 1967 edition} in consideration of the proposition
that `two and two are four' is of the view that this proposition:
\begin{quote}
...states a relationship between the universal `two' and the universal `four'.
\end{quote}
and from which he `endeavours to establish':
\begin{quote}
\textit{All} a priori \textit{knowledge deals exclusively with the relations of universals.}
\end{quote}

\subsection{2001 Sketch Data Models, Relational Schema and Data Specifications, Johnstone and Roseburgh }
\displaybibentry{johnson2001}
I have marked up my paper copy of this paper.

This paper has definitions of EA sketch and of (relational) database schema.
It describes how a database schema can be constructed from an EA sketch and, vice-versa, how an EA sketch can be constructured from a database schema. These constructions are defined as functors betwixt a category of EA sketches and a category of database schemas.

Their definition of database schema has each table with a single primary keys i.e. consisting of a single column rather the more usual one or more columns. As a consequence of this when a round trip is made  
EA sketch $\morph$ database schema $\morph$ EA sketch then the resulting sketch has no commuting diagerams.
In fact no commutimng diagrams are ever produced by the construction  database schema $\morph$ EA sketch.
Things would be far different if composite primary keys and thereby composite foreign keys 
had not been ignored.

Johnson and Rosebrugh use the term `attribute' is used for a node/object which is a coproduct of a number of copies of the terminal object $1$ and use the term `entity' for other node/objects other than $1$ itself.
They say  \textit{sometimes we have insisted that attributes not be the domains of arrows}. It seems they no longer insist on this. 

I do not understand the discussion in  section 4. Normalisation
This section commences with:
\textit{In principle the databae scheme arising from a keyed EA sketch should be close
to being normalised.}

By my reckoning this is just wrong. The EA sketch may well not represent all reachable functional depedencies.


Followed by: \textit{Recall that functional dependencies are specifed from one set of attributes to another}. They don't say where to recall this from. I think they are refering to quite a restricted and in my mind invalid way of framing functional dependencies and not how Codd conceptualised it in 1971.

Their section 7 Conclusion is in the main maybe intended to be word play but it really isn't funny. Don't just confuse do something about it. Don't talk about models of categories or sketches talk about instances.



\subsection{2002 Entity-relationship-attribute designs and
sketches }
\displaybibentry{Johnson2002ERA}
Discussion of overloading of the term`model'.
Introduction of the term `EA-sketch'. 


Algebraic Methodology and Software Technology 12th International Conference Johnson and Rosebrugh page 236
\begin{quote}
An EA sketch E=(G,D,L,C) is a sketch with only finite cones and finite discrete cocones and with a 
specified cone with empty base whose vertex is called 1. Edges with domain 1 are called elements. 
Nodes which are vertices of cocones all of whose injections are elements are called attributes. 
Nodes which are neither attributes, nor 1, are called entitites.
\end{quote}
The theory associated with a sketch is a category of models in a certain 2-category(!).
\begin{quote}
we drop the distinction between entities and relationships. This point of view is also
espoused by other writers on database theory, for example C. J. Date, [7]. It explains
why we have chosen to speak of `EA' sketches rather than `ERA' sketches.
\end{quote}
\begin{quote}
As a special case we mention the `keyed' EA sketches in which every entity has
a specified monomorphism to a key attribute.
\end{quote}
Discrete cocones just means coproducts!

What is the rub with entity types which have elements?

Now the claim is that models have to be in categories which are lextensive -- which is to say
that coproducts and limits behave nicely. sums are universal and disjoint (see [\cite {Carboni1993}])

universal means

disjoint means 
Why is this?

\section{IDL (Carnegie-Melon)}

\subsection{Nestor 1981}
\displaybibentry{Nestor1981}

\begin{quote}
This report defines IDL, a mechanism for specifying properties, of structured data. The objective of this
specification is to permit the data to be communicated safety and efficiently among related programs.
\end{quote}

\subsection{Lamb1997}
\displaybibentry{Lamb1997}
On why you should use IDL:
\begin{quote}
1. IDL can describe complex data structures in a simple but formal way.
It gives you a way to talk about the information content of your data
separately from its representation, much more clearly than most programming 
languages do.
2. IDL tools can automatically generate stereotypical modules of a program,
such as input/output procedures.
3. The automatically-generated modules support passing data between programs 
that might be written in different programming languages, and
might run on different machines.
\end{quote}

Subsequently (cite{Nestor1981}) they describe their model of structured data to be that of `typed attributed directed graphs'. 

\begin{quote}
... the
model must be a very general one... We have
chosen typed, attributed directed graphs as our model. Informally, this domain is a collection of objects.
Each object has a type, a location, and a value. One category of types in the model are node types**. The value
of a node object is a collection of attributes; the particular attributes associated with a node object are a
property of its type. No two attributes of the same node type have the same name; each attribute of a node
object has an associated location. Attributes are also typed; the objects form a graph because some of the
attributes may reference other objects.

** The other categories are scalars (integers, rational booleans. and strings), sets, and sequences.
\end{quote}

and from Lamb 1997 again:
\begin{quote}
Thus the notation encourages general graph structures; to restrict the graph to a tree, you
would need to use the assertion language
\end{quote}

\section{Google Protocol Buffers}
From 
\begin{quote}
Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. 
\end{quote}

These have message structures defined in an IDL-like language. Message types and fields. Unlike Carnegie-Melon IDL Google's proto language restructs to
hierarchical structures i.e trees of node whereas C-M IDL had arbitrary directed graphs. Nodetype is renamed message type and attribute is renamed field.

\begin{quote}
each message type has one or more uniquely numbered fields, and each field has a name and a value type, where value types can be numbers (integer or floating-point), booleans, strings, raw bytes, or ... other protocol buffer message types, allowing you to structure your data hierarchically. You can specify optional fields, required fields, and repeated fields.
\end{quote}
Cannot have aggregations and inheritance but can have \textbf{oneof} fields. This corresponds to exclusion arcs in ER modelling (Barker) or in category theory terms to a 

dual of a multi-category in place of coproducts. 

\section{Comulti categories}
Notes on Higher Categories and Categorical Logic
Lectures by Mike Shulman and Peter LeFanu Lumsdaine;
Notes by Jacob Alexander Gross
August 12, 2016
\begin{quote}
There is a notion of comulticategory, where
arrows have one object in the domain and multiple in the codomain. 
\end{quote}

\section{A. J. Cotnoir}
Strange Parts: The Metaphysics of Non-classical Mereologies

The idea that predefined types are universals and that reference to them is the same as composition:
\begin{quote}
Universals are typically said to be ‘wholly located wherever they are instantiated’.
\end{quote}
and then the very next sentence the idea of dependent universals:
\begin{quote}
Some have thought that universals can have other universals as parts; the locus classicus being
Armstrong (1986).
\end{quote}


\begin{quote}
As water is H2O, the structural universal WATER has the universal HYDROGEN as a
component twice over and the universal OXYGEN as a component once over (and perhaps
we’d like to include two copies of the BONDING universal). By contrast, hydrogen peroxide
is H2O2. Hence, the structural universal HYDROGEN PEROXIDE has the component
HYDROGEN twice over and the component OXYGEN twice over (plus, perhaps, BONDING
thrice over).
\end{quote}
\begin{quote}
Summing together WATER with an extra OXYGEN (and if we include BONDING, an extra
instance of that too) does not yield WATER as idempotence would predict but yields
HYDROGEN PEROXIDE. Hence, either OXYGEN is not part of WATER or idempotence fails.
So, if structural universals were structured by CEM, then WATER and HYDROGEN PEROXIDE
would be identical, but they aren’t.
\end{quote}
\begin{quote}
Lewis (1986) concludes, pace Armstrong, that structural universals do not exist since they
cannot be handled within CEM. And for Lewis, it is either (classical extensional) mereology
or ‘magic’. Bennett (forthcoming) suggests an alternate account, however; one which allows
one to keep a mereological account of structural universals whilst rejecting idempotence.
\end{quote}
\begin{quote}
Bennett’s mereology makes use of the distinction between a role and an occupant of that
role; this allows for the distinction between two types of objects in our domain: ‘parthood
slots’ and the ‘fillers’, which fill those slots. Once it is allowed that objects have this slot structure, it is a small step toward allowing that a single part might fill more than one slot in the
same object. A structural universal like WATER, for example, has three (perhaps five) slots,
two of which are filled by HYDROGEN, and one of which is filled by OXYGEN (and perhaps
two slots filled by BONDING).
\end{quote}
\begin{quote}
Unfortunately, Bennett’s mereology runs into some trouble ...
\end{quote}

\section{Wikipedia BCNF Entry}
This has an example based on the following which has similarity to Zaniolo Example 2.
I need find the original source of this example.
\begin{center}
\input{\erpictureFolder/nearestShop}
\end{center}
Mono sources are:
\newcommand{\attr}[1]{#1}
\renewcommand{\attr}[1]{\psframebox[linecolor=red,framearc=.1]{#1}}
\newcommand{\attrtype}[1]{#1}
\renewcommand{\attrtype}[1]{\psframebox[linecolor=blue,framearc=.1]{#1}}
\newcommand{\etype}[1]{#1}
\renewcommand{\etype}[1]{\psframebox[linecolor=red,framearc=.1]{#1}}
\begin{center}
\input{nearestShopMonoSource1}
\end{center}
and

\begin{center}
\input{nearestShopMonoSource2}
\end{center}

This is a useful example because without the reference relationship the model in not `well-formulated'. With the reference relationship added
then the identifying relationship `nearestShop.ofType' is a derived relationship. Therefore this is a model which cannot be well-formulated.
In the wikipedia article this is given as if adding a relation yields Elementary Key Normal Form but I think this is misleading because 
the table is in EKNF. Rather it is an example of EKNF not being in BCNF.

Seems to me that a truly 'logical' ER model woyuld not represent the composite in the directed graph but specify that the composite as part of a mono source. This way different physical implementations can decide how to implement the composite. Therefore need revised defintuion of EA sketch.




\section{Explaining proper formulation}
Example from 8-15 March 2019 Notebook.
Suppose we are presented with an  initial model which represents items that either have been or will be picked from crates held in various warehouses.
As shown here:
\begin{center}
\input{\erpictureFolder/pickingsInitial}
\end{center}
each `crate' holds items of a certain type and is identified by `crateId'. There is record of 
the `quantity' picked from each crate to satisfy a `delivery'.  

With knowledge of the business or by examination of the data we might find that  within the context 
of entities of type `pick' there is a functional dependency of the 'warehouse' attribute of each  related `crate' entity upon the `deliveryId' attribute.

Formally this can be represented so:
\begin{equation}
\label{warehousefd}
pick: deliveryId \longrightarrow  from[crate]/warehouse
\end{equation}
This functional dependency is not represented in the model and therefore the model cannot be 
considered complete.

As a next step we should introduce an entity identified by the left hand side 
of the fd (\ref{warehousefd})), i.e.
 `deliveryId' and assign to it a `warehouse' attribute as shown here:
\begin{center}
\input{\erpictureFolder/pickingsIntermediate}
\end{center}
This model makes explicit the dependency of warehouse upon deliveryId - the fact that each delivery consists of items picked from a single warehouse. We now have to note however that there is now
an identity of derived attributes of the entity `pick' (warehouse reached from pick via both delivery and crate). Formally:

\begin{equation}
pick:for/warehouse=from/warehouse
\end{equation}

To make this explicit as a commuting diagram of relationships between entities
we need introduce a `warehouse' entity  to reach a final model:

\begin{center}
\input{\erpictureFolder/pickingsFinal}
\end{center}
In this final model the square of relationships commutes as depicted by the scope constraint annotation.

 
\bibliography{../SharedBibliography/temp/bibliography}

\end{document}